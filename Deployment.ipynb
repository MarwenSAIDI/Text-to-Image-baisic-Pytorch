{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c562a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ecf89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb9df70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55d81b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_imgs(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            (\"conv1\", nn.Conv1d(720,500,64)),\n",
    "            (\"relu1\", nn.ReLU()),\n",
    "            (\"dropout1\", nn.Dropout(0.1)),\n",
    "            (\"conv2\", nn.Conv1d(500,200,8)),\n",
    "            (\"relu2\", nn.ReLU()),\n",
    "            (\"dropout2\", nn.Dropout(0.1)),\n",
    "            (\"conv3\", nn.Conv1d(200,50,1)),\n",
    "            (\"linear\", nn.Linear(410,20))\n",
    "        ]))\n",
    "\n",
    "        self.decoder = nn.Sequential(OrderedDict([\n",
    "            (\"linear\", nn.Linear(20,410)),\n",
    "            (\"conv2\", nn.Conv1d(50,200,1)),\n",
    "            (\"dropout2\", nn.Dropout(0.1)),\n",
    "            (\"relu2\", nn.ReLU()),\n",
    "            (\"conv3\", nn.ConvTranspose1d(200,500,8)),\n",
    "            (\"dropout1\", nn.Dropout(0.1)),\n",
    "            (\"relu3\", nn.ReLU()),\n",
    "            (\"conv4\", nn.ConvTranspose1d(500,720,64)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "class AE_corpus(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            (\"lin1\", nn.Linear(2400, 1000)),\n",
    "            (\"lin2\", nn.Linear(1000, 500)),\n",
    "            (\"dp\", nn.Dropout(.2)),\n",
    "            (\"lin3\", nn.Linear(500, 100)),\n",
    "            (\"lin4\", nn.Linear(100, 50)),\n",
    "            (\"linear\", nn.Linear(50,20)),\n",
    "        ]))\n",
    "\n",
    "        self.decoder = nn.Sequential(OrderedDict([\n",
    "            (\"linear\", nn.Linear(20,50)),\n",
    "            (\"lin4\", nn.Linear(50, 100)),\n",
    "            (\"lin3\", nn.Linear(100, 500)),\n",
    "            (\"dp\", nn.Dropout(.2)),\n",
    "            (\"lin2\", nn.Linear(500, 1000)),\n",
    "            (\"lin1\", nn.Linear(1000, 2400)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "class CLIP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## the shape of the input is (1,20). And the output is (50, 20)\n",
    "        self.translator = nn.Sequential(OrderedDict([\n",
    "            (\"conv1\", nn.ConvTranspose1d(1,50,3)),\n",
    "#             ('relu1', nn.ReLU()),\n",
    "            ('dout1', nn.Dropout(.1)),\n",
    "            (\"lin1\", nn.Linear(22,20)),\n",
    "            ('dout2', nn.Dropout(.1)),\n",
    "#             ('relu2', nn.ReLU()),\n",
    "            (\"lin2\", nn.Linear(20,30)),\n",
    "#             ('relu3', nn.ReLU()),\n",
    "            (\"lin3\", nn.Linear(30,50)),\n",
    "#             ('relu4', nn.ReLU()),\n",
    "            (\"lin4\", nn.Linear(50,20)),\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        trans = self.translator(x)\n",
    "        \n",
    "        return trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d99c8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model_img = AE_imgs()\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    " \n",
    "# Using an Adam Optimizer with lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model_img.parameters(),\n",
    "                             lr = .0001, weight_decay = .0005)\n",
    "# Load the model\n",
    "model_img = torch.load(\"MLModels/AE_images\")\n",
    "model_img = model_img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d88ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model_corpus = AE_corpus()\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    " \n",
    "# Using an Adam Optimizer with lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model_corpus.parameters(),\n",
    "                             lr = .0001, weight_decay = .0001)\n",
    "# Load the model\n",
    "model_corpus = torch.load(\"MLModels/AE_corpus\")\n",
    "model_corpus = model_corpus.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbe8a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model_clip = CLIP()\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    " \n",
    "# Using an Adam Optimizer with lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model_clip.parameters(),\n",
    "                             lr = .01, weight_decay = .0001)\n",
    "# Load the model\n",
    "model_clip = torch.load(\"MLModels/CLIP\")\n",
    "model_clip = model_clip.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8b618d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = pickle.load(open(\"MLModels/Vectorizer.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca358268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipline(text):\n",
    "    corpus = np.array([text])\n",
    "    features = cv.transform(corpus)\n",
    "    vectors = np.zeros((features.toarray().shape[0], features.toarray().shape[1], 50))\n",
    "    for i in range(features.toarray().shape[0]):\n",
    "        copy = features.toarray()[i]\n",
    "        for idx in np.argwhere(copy).reshape(1,-1)[0]:\n",
    "            vectors[i][idx] = glove.get_vector(cv.get_feature_names()[idx])\n",
    "            \n",
    "    vect_ten = []\n",
    "    for i in range(vectors.shape[0]):\n",
    "        vect_ten.append(vectors[i])\n",
    "    vect_ten = torch.tensor(np.array(vect_ten))\n",
    "    vect_ten = vect_ten.type(torch.FloatTensor)\n",
    "    return vect_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "002b80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_image(vect_ten):\n",
    "    enc = model_corpus.encoder(vect_ten[0].reshape(1,-1))\n",
    "    translate = model_clip(enc)\n",
    "    image = model_img.decoder(translate)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e77763",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc7f9dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description: Painting of the Monalisa in modern art.\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Description: \")\n",
    "vect_ten = pipline(text)\n",
    "vect_ten = vect_ten.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff33a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_image = text_image(vect_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19d05175",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(array_image.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5c786359",
   "metadata": {},
   "outputs": [],
   "source": [
    "im.convert('RGB').save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b228049b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
